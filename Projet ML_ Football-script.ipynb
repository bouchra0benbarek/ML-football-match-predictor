{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9aafb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Read training data\n",
    "train_home_team_statistics_df = pd.read_csv('/Train_Data/train_home_team_statistics_df.csv', index_col=0)\n",
    "train_away_team_statistics_df = pd.read_csv('/Train_Data/train_away_team_statistics_df.csv', index_col=0)\n",
    "train_scores = pd.read_csv('/Y_train_football.csv', index_col=0)\n",
    "\n",
    "# Data preparation\n",
    "train_home = train_home_team_statistics_df.iloc[:, 2:]\n",
    "train_away = train_away_team_statistics_df.iloc[:, 2:]\n",
    "\n",
    "# Rename columns\n",
    "train_home.columns = 'HOME_' + train_home.columns\n",
    "train_away.columns = 'AWAY_' + train_away.columns\n",
    "\n",
    "# Reassemble data in one dataset\n",
    "train_data = pd.concat([train_home, train_away], join='inner', axis=1)\n",
    "train_scores = train_scores.loc[train_data.index]\n",
    "\n",
    "# Replace infinite data with NaN\n",
    "train_data = train_data.replace({np.inf: np.nan, -np.inf: np.nan})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b1d54",
   "metadata": {},
   "source": [
    "# Data preprocessing & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9717619",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a03fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13185c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f08670",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ca180",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46217557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_scores\n",
    "counts = df.apply(pd.Series.value_counts).T\n",
    "counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Target variable distribution')\n",
    "plt.xlabel('Classes (HOME_WINS-DRAW-AWAY_WINS)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e849fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round((train_data.isnull().sum()/train_data.shape[0])*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5504dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with the mean value of each feature \n",
    "train_data = train_data.fillna(train_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e29685d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalisation\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "04c7ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_home_scaled = scaler.fit_transform(train_home)\n",
    "train_away_scaled = scaler.fit_transform(train_away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b900b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_home_scaled = train_data.fillna(train_home.mean())\n",
    "train_away_scaled = train_data.fillna(train_away.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean value after z-score', train_data_scaled.mean())\n",
    "print('Standard deviation value after z-score', train_data_scaled.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_data_scaled).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = train_data.corrwith(train_scores['HOME_WINS'])\n",
    "print(correlations.max())\n",
    "correlations = train_data.corrwith(train_scores['AWAY_WINS'])\n",
    "print(correlations.max())\n",
    "correlations = train_data.corrwith(train_scores['DRAW'])\n",
    "print(correlations.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c2983",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train_data = train_data.corr()\n",
    "corr_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_train_data, annot=False, cmap='coolwarm', fmt='.2f')\n",
    "\n",
    "plt.title(\"train data features Correlation Matrix\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6472507",
   "metadata": {},
   "source": [
    "# Splitting data into train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0a47167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "train_new_y = train_scores[['HOME_WINS', 'DRAW', 'AWAY_WINS']]\n",
    "\n",
    "# Data splitting to train, validation and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(train_data_scaled, train_new_y, train_size=0.8, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X_train, y_train, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2fdc2",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST parameters\n",
    "params_1 = {\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'hist',\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.025,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,  # Trois classes : HOME_WINS, DRAW, AWAY_WINS\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "original_columns = np.array(train_data_selected.columns).tolist()\n",
    "\n",
    "# DMatrix for XGBoost\n",
    "d_train = xgb.DMatrix(X_train, label=np.argmax(y_train.values, axis=1), feature_names=original_columns)  \n",
    "d_valid = xgb.DMatrix(X_valid, label=np.argmax(y_valid.values, axis=1), feature_names=original_columns)\n",
    "\n",
    "# Early stopping\n",
    "num_round = 10000\n",
    "evallist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "# Model training\n",
    "bst = xgb.train(params_1, d_train, num_round, evallist, early_stopping_rounds=100, evals_result=evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the log-loss values for train and validation from evals_result\n",
    "train_logloss = evals_result['train']['mlogloss']\n",
    "valid_logloss = evals_result['eval']['mlogloss']\n",
    "\n",
    "# Plot the log-loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_logloss, label='Train Log-Loss', color='blue')\n",
    "plt.plot(valid_logloss, label='Validation Log-Loss', color='red')\n",
    "plt.xlabel('Number of Rounds')\n",
    "plt.ylabel('Log-Loss')\n",
    "plt.title('Log-Loss during Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "importance = bst.get_score(importance_type='weight')\n",
    "sorted_dict = sorted(importance.items(), key=lambda item: item[1], reverse=True)\n",
    "dict(sorted_dict[:100]).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ec57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'importance des fonctionnalités\n",
    "xgb.plot_importance(bst, max_num_features=100)\n",
    "plt.gcf().set_size_inches(15, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb11bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss')\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'max_depth': [4, 5],\n",
    "    'learning_rate': [0.025, 0.1],\n",
    "    'n_estimators': [150, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8,0.9,1.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1)\n",
    "grid_search.fit(X_train, np.argmax(y_train.values, axis=1))\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6de4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# Predictions on test dataset\n",
    "X_test_xgb = xgb.DMatrix(X_test)\n",
    "predictions = bst.predict(X_test_xgb, iteration_range=(0, bst.best_iteration))\n",
    "\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(np.argmax(y_test.values, axis=1), predicted_classes)\n",
    "f1 = f1_score(np.argmax(y_test.values, axis=1), predicted_classes, average='micro')\n",
    "precision = precision_score(np.argmax(y_test.values, axis=1), predicted_classes, average='micro')\n",
    "f1 = f1_score(np.argmax(y_test.values, axis=1), predicted_classes, average='micro')\n",
    "recall = recall_score(np.argmax(y_test.values, axis=1), predicted_classes, average='micro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"f1 score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75d6eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soumission des prédictions\n",
    "test_home = pd.read_csv('/Test_Data/test_home_team_statistics_df.csv', index_col=0)\n",
    "test_away = pd.read_csv('/Test_Data/test_away_team_statistics_df.csv', index_col=0)\n",
    "\n",
    "test_home.columns = 'HOME_' + test_home.columns\n",
    "test_away.columns = 'AWAY_' + test_away.columns\n",
    "\n",
    "test_data = pd.concat([test_home, test_away], join='inner', axis=1)\n",
    "test_data_imputed = imputer.transform(test_data)\n",
    "test_data_scaled = scaler.transform(test_data_imputed)\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "X_bench = xgb.DMatrix(test_data_scaled)\n",
    "predictions = bst.predict(X_bench, iteration_range=(0, bst.best_iteration))\n",
    "\n",
    "# Sélection de la classe avec la probabilité la plus élevée\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Création d'un DataFrame pour les résultats\n",
    "submission = pd.DataFrame(predicted_classes, columns=['Prediction'])\n",
    "submission['HOME_WINS'] = (submission['Prediction'] == 0).astype(int)\n",
    "submission['DRAW'] = (submission['Prediction'] == 1).astype(int)\n",
    "submission['AWAY_WINS'] = (submission['Prediction'] == 2).astype(int)\n",
    "\n",
    "# Sauvegarde des résultats de la soumission\n",
    "submission = submission[['HOME_WINS', 'DRAW', 'AWAY_WINS']]\n",
    "submission.index = test_data.index\n",
    "submission.reset_index(inplace=True)\n",
    "submission.to_csv('/benchmark_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa237da1",
   "metadata": {},
   "source": [
    "# RandomForest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "# Initialize RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, np.argmax(y_train.values, axis=1))\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(np.argmax(y_valid.values, axis=1), y_pred)\n",
    "print(f\"Random Forest Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e955cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Perform cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Cross-validation score (using accuracy)\n",
    "cv_scores = cross_val_score(rf_model, X_train, np.argmax(y_train.values, axis=1), cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard Deviation of Cross-Validation Accuracy: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the model with the best parameters found by grid search\n",
    "grid_search.fit(X_train, np.argmax(y_train.values, axis=1))\n",
    "\n",
    "# Output the best parameters and best score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fcbdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_home = pd.read_csv('/Test_Data/test_home_team_statistics_df.csv', index_col=0)\n",
    "test_away = pd.read_csv('/Test_Data/test_away_team_statistics_df.csv', index_col=0)\n",
    "\n",
    "test_home.columns = 'HOME_' + test_home.columns\n",
    "test_away.columns = 'AWAY_' + test_away.columns\n",
    "\n",
    "test_data = pd.concat([test_home, test_away], join='inner', axis=1)\n",
    "test_data_cleaned = test_data.fillna(test_data.mean())\n",
    "scaler = StandardScaler()\n",
    "test_data_scaled = scaler.fit_transform(test_data_cleaned)\n",
    "\n",
    "predictions = rf_model.predict(test_data_scaled)\n",
    "\n",
    "submission = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "submission['HOME_WINS'] = (submission['Prediction'] == 0).astype(int)\n",
    "submission['DRAW'] = (submission['Prediction'] == 1).astype(int)\n",
    "submission['AWAY_WINS'] = (submission['Prediction'] == 2).astype(int)\n",
    "\n",
    "submission = submission[['HOME_WINS', 'DRAW', 'AWAY_WINS']]\n",
    "submission.index = test_data.index\n",
    "submission.reset_index(inplace=True)\n",
    "submission.to_csv('/benchmark_submission_randomforest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1df30e",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree model parameters\n",
    "dt_params = {\n",
    "    'max_depth': 3,\n",
    "    'min_samples_split': 2,\n",
    "    'criterion': 'entropy', \n",
    "    'random_state': 42, \n",
    "    'max_features': 'auto', \n",
    "    'min_samples_leaf': 5, \n",
    "    'min_samples_split': 18, \n",
    "    'splitter': 'random'\n",
    "}\n",
    "\n",
    "dt_model = DecisionTreeClassifier(**dt_params)\n",
    "\n",
    "# Train the model on the training data\n",
    "dt_model.fit(X_train, np.argmax(y_train.values, axis=1))\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = dt_model.predict(X_valid)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(np.argmax(y_valid.values, axis=1), y_pred)\n",
    "\n",
    "print(f\"Decision Tree Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4488e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "precision = precision_score(np.argmax(y_valid.values, axis=1), y_pred, average='weighted')\n",
    "recall = recall_score(np.argmax(y_valid.values, axis=1), y_pred, average='weighted')\n",
    "f1 = f1_score(np.argmax(y_valid.values, axis=1), y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(np.argmax(y_valid.values, axis=1), y_pred)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab641f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Perform cross-validation for multiple metrics\n",
    "cv_results = cross_validate(dt_model, X_train, np.argmax(y_train.values, axis=1), cv=5, \n",
    "                            scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(f\"Accuracy: {cv_results['test_accuracy']}\")\n",
    "print(f\"Precision (Weighted): {cv_results['test_precision_weighted']}\")\n",
    "print(f\"Recall (Weighted): {cv_results['test_recall_weighted']}\")\n",
    "print(f\"F1-Score (Weighted): {cv_results['test_f1_weighted']}\")\n",
    "print(f\"Mean Accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "print(f\"Mean Precision: {cv_results['test_precision_weighted'].mean()}\")\n",
    "print(f\"Mean Recall: {cv_results['test_recall_weighted'].mean()}\")\n",
    "print(f\"Mean F1-Score: {cv_results['test_f1_weighted'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'max_depth': randint(1, 20),               \n",
    "    'min_samples_split': randint(2, 20),       \n",
    "    'min_samples_leaf': randint(1, 20),        \n",
    "    'max_features': ['auto', 'sqrt', 'log2'], \n",
    "    'criterion': ['gini', 'entropy'],         \n",
    "    'splitter': ['best', 'random']            \n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt_model, \n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,                     \n",
    "    cv=5,                           \n",
    "    verbose=1,                      \n",
    "    random_state=42,                \n",
    "    n_jobs=-1                      \n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, np.argmax(y_train.values, axis=1))\n",
    "\n",
    "# Get the best hyperparameters and the best score\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f063878",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_home = pd.read_csv('/Test_Data/test_home_team_statistics_df.csv', index_col=0)\n",
    "test_away = pd.read_csv('/Test_Data/test_away_team_statistics_df.csv', index_col=0)\n",
    "\n",
    "test_home.columns = 'HOME_' + test_home.columns\n",
    "test_away.columns = 'AWAY_' + test_away.columns\n",
    "\n",
    "test_data = pd.concat([test_home, test_away], join='inner', axis=1)\n",
    "test_data_scaled = test_data.fillna(test_data.mean())\n",
    "test_data_scaled = scaler.transform(test_data_scaled)\n",
    "\n",
    "predictions = dt_model.predict(test_data_scaled)\n",
    "\n",
    "submission = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "submission['HOME_WINS'] = (submission['Prediction'] == 0).astype(int)\n",
    "submission['DRAW'] = (submission['Prediction'] == 1).astype(int)\n",
    "submission['AWAY_WINS'] = (submission['Prediction'] == 2).astype(int)\n",
    "\n",
    "submission = submission[['HOME_WINS', 'DRAW', 'AWAY_WINS']]\n",
    "submission.index = test_data.index\n",
    "submission.reset_index(inplace=True)\n",
    "submission.to_csv('/benchmark_submission_decisiontree.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
